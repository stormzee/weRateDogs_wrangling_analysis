{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling report for WeRateDogs Project\n",
    "* By: Samuel Addo Oppong\n",
    "* Date: September 4, 2022 <br>\n",
    "\n",
    "Data wrnagling with the WeRateDogs dataset was so much fun and challenging at the same time. This project had me spending a whole week reading and learning how to handle this particular dataset. This project has thought me so much that i did not know at the start of this nanodegree.\n",
    "\n",
    "During the start of this project, i only had at my disposal am unclean twitter_archives dataset which i was made to download manually into my proejct workspace. Again, the instructions were that, I had to programmatically donwload another dataset into my project workspace. During this second download, i used the capabilities of the python requests library to enable me download the second data to be able to work with it. At this point, I had two datasets in my project workspace and waiting to download the third and this is where the challenge started for me. The third dataset had me use the tweepy library to get data from the twitter API. Before I could do this, i needed to signup for a developer account to be able to use the twitter API to download the additional data i needed for this project. I went through the steps one at a time until the end and only to find out that, my developer account could not be activated for me to use the API to get the data i needed from the twitter API. Due to the fact that time is of the essence and also not on my side, I had to use the url to the one(additional data) provided by Udacity so as to get things done in time for my project submission. Therefore, I again used the programmatic method of downloading files and got the file into my project workspace. Now this file was in a `.txt` format in which the actual content was json content and needed to be converted into a pandas dataframe for use with the Pandas library. Here, I read the `tweet_json.txt` file and then first converted it to a json file format, then I read the file line by line so as to obtain the `retweet counts` and `favorite counts` of the tweets and stored in a list of json objects(dictionaries) which were later used to create a pandas dataframe. At this point, I had all the three(3) datasets ready to be used in my project workspace. \n",
    "\n",
    "Now, the main tasks in this project were to identify, resolve and clean data quality and data tidynes issues found in these datasets. A lot of the issues were just be ided through visual assessment or by just looking at some portions of the dataset. A couple of pandas functions like head, tail and sample also came in handy. Some of the issues needed a little Others required a little more analysis/investigations, mainly through summaries or filtering out certain sections of the data. The `.info` and `.value_counts` functions were mainly used. Most of the tidiness issues involved joining of the tables, extracting certain key variables from already existing ones and also melting certain features into a single column.\n",
    "\n",
    "Also, revealing some quality and tidyness issues about the dataset, it became necessary to perform data cleaning on the identified issues(columns/rows). During this cleaning stage, the declaration of the issue was pertinent as well as the define, code and test framework. During cleaning, some columns needed to be dropped, features needed to be extracted using regex and replacing and then data finally merged into a single dataset. This formality made it much easier to thoroughly clean the data for analysis, insights driving and visualizations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
